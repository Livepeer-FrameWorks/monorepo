---
title: "Deployment Overview"
description: "Comprehensive guide to deploying FrameWorks on bare metal, including architecture, CLI usage, and operations."
sidebar:
  order: 5
---

import { Card, CardGrid, LinkCard, Aside } from "@astrojs/starlight/components";

<Aside type="caution" title="Experimental Feature">
  Self-hosting and the FrameWorks CLI are currently in **active testing**. APIs and configuration
  formats may change. Proceed with caution for production environments.
</Aside>

Deploying FrameWorks gives you full ownership of your video infrastructure.

## Quick Start

**Production Deployment (Recommended):**

```bash
# 1. Install CLI
cd cli/ && go build -o frameworks && sudo mv frameworks /usr/local/bin/

# 2. Review prerequisites
# See: /operators/prerequisites

# 3. Create cluster manifest (define clusters for multi-cluster topologies)
# See: /operators/cluster-manifest

# 4. Deploy infrastructure
frameworks cluster provision --manifest cluster.yaml --only infrastructure

# 5. Initialize databases
frameworks cluster init all --manifest cluster.yaml

# 6. Deploy applications
frameworks cluster provision --manifest cluster.yaml --only applications

# 7. Verify health
frameworks cluster doctor --manifest cluster.yaml
```

**Local Development:**

```bash
# Quick local stack with Docker Compose
cd monorepo
cp config/env/secrets.env.example config/env/secrets.env  # First time only
make env                  # Generate .env from layered config
docker-compose up -d
```

See root `README.md` for local endpoints and ports.

<Aside type="tip">
  For multi-cluster deployments (separate control and media planes, multiple regions), define a
  `clusters` section in your manifest. The CLI creates all clusters, registers nodes under the
  correct cluster, and injects `CLUSTER_ID` into each service. See the [Cluster Manifest
  Reference](/operators/cluster-manifest#clusters) for details.
</Aside>

## Deployment Paths

### 1. CLI Deployment (Recommended)

The FrameWorks CLI automates the complex orchestration of multi-tier infrastructure, handling everything from SSH keys to database migrations (via internal provisioners; Ansible is used for some infrastructure components).

**Features:**

- Multi-host SSH deployment
- Docker and native (systemd) support
- GitOps version management
- Automated health validation
- Backup/restore operations
- Comprehensive diagnostics

<CardGrid>
  <LinkCard
    title="CLI Reference"
    href="/operators/cli-reference"
    description="Complete CLI command documentation."
  />
  <LinkCard
    title="Cluster Manifest"
    href="/operators/cluster-manifest"
    description="Configure your deployment."
  />
  <LinkCard
    title="Prerequisites"
    href="/operators/prerequisites"
    description="System requirements and setup."
  />
  <LinkCard
    title="External Services"
    href="/operators/external-services"
    description="Third-party integrations."
  />
</CardGrid>

### 2. Manual Deployment

For operators who cannot or choose not to use the CLI:

- [Manual Deployment](/operators/deployment-manual) - Step-by-step bare metal deployment guide
- [Operations Reference](/operators/operations) - Post-deployment operations, debugging, tuning
- [WireGuard Mesh Setup](/operators/wireguard)

## Architecture

For detailed architecture documentation including service tables, ports, deployment tiers, and data flow diagrams, see [Architecture Overview](/operators/architecture).

## Security

### Network Security

- Backend nodes (central/regional) communicate over a private mesh (WireGuard or equivalent). Edge nodes do not join the mesh.
- When running more than one backend node, a private mesh is required for databases, Kafka, and service-to-service calls.
- TLS termination varies by tier:
  - Development: Nginx reverse proxy (optional TLS)
  - Edge nodes: Caddy (automatic HTTPS via HTTP-01)
  - Central/Regional: Navigator-issued certificates (DNS-01) or operator-managed proxies
- Firewall rules restrict access to required ports

### Data Security

- TLS at ingress (Caddy/Navigator/proxies) should be enabled; internal gRPC TLS is not enabled yet (RFC draft exists).
- Kafka inter-broker encryption is not enabled yet; the mesh provides the current security boundary.
- Service-to-service authentication uses `SERVICE_TOKEN` (RFC draft exists for stronger service identity).
- Secrets are managed via environment variables (Vault RFC draft exists).

### Access Control

- Multi-tenant isolation at application layer
- User roles exist, but API tokens are currently full-access; RBAC and finer permissions are planned.
- API rate limiting is enforced at the gateway.
- SSH key-based authentication for deployments

For monitoring, operations, and CLI commands, see [CLI Reference](/operators/cli-reference) and [Operations](/operators/operations).
