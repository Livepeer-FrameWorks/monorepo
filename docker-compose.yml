# FrameWorks Development Environment
# Boots all the microservices, interfaces, media server, etc.

networks:
  frameworks:
    driver: bridge

volumes:
  postgres_data:
  foghorn_redis_data:
  mistserver_data:
  zookeeper_data:
  zookeeper_logs:
  kafka_data:
  clickhouse_data:
  platform_redis_data:

x-default-env: &default-env
  env_file:
    - ${ENV_FILE:-.env}
  environment:
    GRPC_METADATA_POLICY: ${GRPC_METADATA_POLICY}

services:
  # Bridge - GraphQL API Gateway
  bridge:
    <<: *default-env
    container_name: frameworks-bridge
    build:
      context: .
      dockerfile: api_gateway/Dockerfile
    environment:
      BUILD_ENV: ${BUILD_ENV}
      PORT: ${BRIDGE_PORT}
      LOG_LEVEL: ${LOG_LEVEL}
      GIN_MODE: ${GIN_MODE}
      GRAPHQL_PLAYGROUND_ENABLED: ${GRAPHQL_PLAYGROUND_ENABLED}
      GRAPHQL_COMPLEXITY_LIMIT: ${GRAPHQL_COMPLEXITY_LIMIT}
      TRUSTED_PROXY_CIDRS: ${TRUSTED_PROXY_CIDRS}
      WEBHOOK_RATE_LIMIT_PER_MIN: ${WEBHOOK_RATE_LIMIT_PER_MIN}
      # gRPC addresses (internal service-to-service)
      COMMODORE_GRPC_ADDR: ${COMMODORE_GRPC_ADDR}
      QUARTERMASTER_GRPC_ADDR: ${QUARTERMASTER_GRPC_ADDR}
      PURSER_GRPC_ADDR: ${PURSER_GRPC_ADDR}
      PERISCOPE_GRPC_ADDR: ${PERISCOPE_GRPC_ADDR}
      SIGNALMAN_GRPC_ADDR: ${SIGNALMAN_GRPC_ADDR}
      SKIPPER_GRPC_ADDR: ${SKIPPER_GRPC_ADDR:-skipper:19007}
      WS_MAX_CONNECTIONS_PER_TENANT: ${WS_MAX_CONNECTIONS_PER_TENANT}
      JWT_SECRET: ${JWT_SECRET}
      SERVICE_TOKEN: ${SERVICE_TOKEN}
      USAGE_HASH_SECRET: ${USAGE_HASH_SECRET}
      CLUSTER_ID: ${CLUSTER_ID}
      SKIPPER_SPOKE_URL: http://skipper:18018/mcp/spoke
    ports:
      - "18000:18000"
    depends_on:
      quartermaster:
        condition: service_healthy
      commodore:
        condition: service_started
      periscope-query:
        condition: service_started
      purser:
        condition: service_started
    networks:
      - frameworks
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test:
        ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:18000/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # PostgreSQL
  postgres:
    <<: *default-env
    image: pgvector/pgvector:pg15
    container_name: frameworks-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      # Schema
      - ./pkg/database/sql/schema/quartermaster.sql:/docker-entrypoint-initdb.d/10_init_quartermaster.sql:ro
      - ./pkg/database/sql/schema/purser.sql:/docker-entrypoint-initdb.d/20_init_purser.sql:ro
      - ./pkg/database/sql/schema/foghorn.sql:/docker-entrypoint-initdb.d/30_init_foghorn.sql:ro
      - ./pkg/database/sql/schema/commodore.sql:/docker-entrypoint-initdb.d/40_init_commodore.sql:ro
      - ./pkg/database/sql/schema/periscope.sql:/docker-entrypoint-initdb.d/50_init_periscope.sql:ro
      - ./pkg/database/sql/schema/listmonk.sql:/docker-entrypoint-initdb.d/60_init_listmonk.sql:ro
      - ./pkg/database/sql/schema/navigator.sql:/docker-entrypoint-initdb.d/70_init_navigator.sql:ro
      - ./pkg/database/sql/schema/chatwoot.sql:/docker-entrypoint-initdb.d/75_init_chatwoot.sql:ro
      - ./pkg/database/sql/schema/skipper.sql:/docker-entrypoint-initdb.d/78_init_skipper.sql:ro
      # Static Seeds (Prod)
      - ./pkg/database/sql/seeds/static/purser_tiers.sql:/docker-entrypoint-initdb.d/80_seed_purser_tiers.sql:ro
      # Demo Seeds (Dev/Demo only - loaded by default in this compose profile)
      - ./pkg/database/sql/seeds/demo/demo_data.sql:/docker-entrypoint-initdb.d/99_init_demo.sql:ro
      # - ./database/migrations:/docker-entrypoint-initdb.d/migrations:ro
    ports:
      - "5432:5432"
    networks:
      - frameworks
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    mem_limit: 1g
    mem_reservation: 512m
    cpus: 2.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U frameworks_user -d frameworks"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # MistServer with AV libs.
  # Note: MistServer requires significant shared memory for holding stream buffers.
  mistserver:
    image: livepeerframeworks/mistserver:latest
    # build:
    #   context: ../mistserver
    #   dockerfile: Dockerfile.mistserver
    #   args:
    #   - VERSION=local-dev
    #   - DEBUG=4
    container_name: frameworks-mistserver
    platform: linux/amd64
    shm_size: "2gb" # Allocate 2GB of shared memory for MistServer
    ports:
      - "4242:4242" # Controller
      - "8080:8080" # HTTP
      - "1935:1935" # RTMP
      - "5554:5554" # RTSP
      - "4200:4200" # DTSC (Mist internal format; used for stream replication)
      - "8889:8889/udp" # TS-SRT
      - "18203:18203/udp" # WebRTC
    volumes:
      - ./infrastructure/mistserver.conf:/etc/mistserver.conf
      - mistserver_data:/var/lib/mistserver
      # Demo recordings for dev environment (clips & DVR artifacts)
      - ./infrastructure/demo-recordings:/var/lib/mistserver/recordings
    restart: unless-stopped
    command: ["MistController", "-c", "/etc/mistserver.conf"]
    networks:
      - frameworks
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    mem_limit: 2g
    mem_reservation: 1g
    cpus: 2.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # Foghorn Load Balancer (Regional/Central)
  foghorn:
    <<: *default-env
    # MistServer Load Balancer (comment out to use FrameWorks Foghorn)
    # image: ddvtech/mistserver_loadbalancer:latest
    # command: ["/usr/bin/MistUtilLoad", "-g", "5", "-P", "koekjes", "-p", "8042", "-s", "http://mistserver:4242"]

    # FrameWorks Foghorn
    container_name: frameworks-foghorn
    build:
      context: .
      dockerfile: api_balancing/Dockerfile
    environment:
      DATABASE_URL: ${DATABASE_URL}
      PORT: ${FOGHORN_PORT}
      FOGHORN_CONTROL_BIND_ADDR: ${FOGHORN_CONTROL_BIND_ADDR}
      LOG_LEVEL: ${LOG_LEVEL}
      GIN_MODE: ${GIN_MODE}
      # gRPC addresses (internal)
      COMMODORE_GRPC_ADDR: ${COMMODORE_GRPC_ADDR}
      QUARTERMASTER_GRPC_ADDR: ${QUARTERMASTER_GRPC_ADDR}
      # Cache settings
      COMMODORE_CACHE_TTL: ${COMMODORE_CACHE_TTL}
      COMMODORE_CACHE_SWR: ${COMMODORE_CACHE_SWR}
      COMMODORE_CACHE_NEG_TTL: ${COMMODORE_CACHE_NEG_TTL}
      COMMODORE_CACHE_MAX: ${COMMODORE_CACHE_MAX}
      GEOIP_MMDB_PATH: ${GEOIP_MMDB_PATH}
      GEOIP_CACHE_TTL: ${GEOIP_CACHE_TTL}
      GEOIP_CACHE_SWR: ${GEOIP_CACHE_SWR}
      GEOIP_CACHE_NEG_TTL: ${GEOIP_CACHE_NEG_TTL}
      GEOIP_CACHE_MAX: ${GEOIP_CACHE_MAX}
      DECKLOG_GRPC_ADDR: ${DECKLOG_GRPC_ADDR}
      DECKLOG_USE_TLS: ${DECKLOG_USE_TLS}
      # gRPC TLS cert override (auto-provisions from Navigator when empty)
      GRPC_TLS_CERT_PATH: ${FOGHORN_GRPC_TLS_CERT_PATH}
      GRPC_TLS_KEY_PATH: ${FOGHORN_GRPC_TLS_KEY_PATH}
      FOGHORN_ALLOW_INSECURE_CONTROL_GRPC: "true"
      # Load balancing weights
      CPU_WEIGHT: ${CPU_WEIGHT}
      RAM_WEIGHT: ${RAM_WEIGHT}
      BANDWIDTH_WEIGHT: ${BANDWIDTH_WEIGHT}
      GEO_WEIGHT: ${GEO_WEIGHT}
      STREAM_BONUS: ${STREAM_BONUS}
      SERVICE_TOKEN: ${SERVICE_TOKEN}
      CLUSTER_ID: ${CLUSTER_ID}
      REDIS_URL: ${FOGHORN_REDIS_URL}
      FOGHORN_INSTANCE_ID: foghorn-1
    ports:
      - "18008:18008"
      - "18019:18019"
    depends_on:
      postgres:
        condition: service_healthy
      quartermaster:
        condition: service_healthy
      foghorn-redis:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - frameworks
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test:
        ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:18008/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Foghorn HA peer (overrides instance ID and ports)
  foghorn-2:
    <<: *default-env
    container_name: frameworks-foghorn-2
    build:
      context: .
      dockerfile: api_balancing/Dockerfile
    environment:
      DATABASE_URL: ${DATABASE_URL}
      PORT: 18018
      FOGHORN_CONTROL_BIND_ADDR: 0.0.0.0:18029
      LOG_LEVEL: ${LOG_LEVEL}
      GIN_MODE: ${GIN_MODE}
      COMMODORE_GRPC_ADDR: ${COMMODORE_GRPC_ADDR}
      QUARTERMASTER_GRPC_ADDR: ${QUARTERMASTER_GRPC_ADDR}
      COMMODORE_CACHE_TTL: ${COMMODORE_CACHE_TTL}
      COMMODORE_CACHE_SWR: ${COMMODORE_CACHE_SWR}
      COMMODORE_CACHE_NEG_TTL: ${COMMODORE_CACHE_NEG_TTL}
      COMMODORE_CACHE_MAX: ${COMMODORE_CACHE_MAX}
      GEOIP_MMDB_PATH: ${GEOIP_MMDB_PATH}
      GEOIP_CACHE_TTL: ${GEOIP_CACHE_TTL}
      GEOIP_CACHE_SWR: ${GEOIP_CACHE_SWR}
      GEOIP_CACHE_NEG_TTL: ${GEOIP_CACHE_NEG_TTL}
      GEOIP_CACHE_MAX: ${GEOIP_CACHE_MAX}
      DECKLOG_GRPC_ADDR: ${DECKLOG_GRPC_ADDR}
      DECKLOG_USE_TLS: ${DECKLOG_USE_TLS}
      GRPC_TLS_CERT_PATH: ${FOGHORN_GRPC_TLS_CERT_PATH}
      GRPC_TLS_KEY_PATH: ${FOGHORN_GRPC_TLS_KEY_PATH}
      FOGHORN_ALLOW_INSECURE_CONTROL_GRPC: "true"
      CPU_WEIGHT: ${CPU_WEIGHT}
      RAM_WEIGHT: ${RAM_WEIGHT}
      BANDWIDTH_WEIGHT: ${BANDWIDTH_WEIGHT}
      GEO_WEIGHT: ${GEO_WEIGHT}
      STREAM_BONUS: ${STREAM_BONUS}
      SERVICE_TOKEN: ${SERVICE_TOKEN}
      CLUSTER_ID: ${CLUSTER_ID}
      REDIS_URL: ${FOGHORN_REDIS_URL}
      FOGHORN_INSTANCE_ID: foghorn-2
    ports:
      - "18028:18018"
      - "18029:18029"
    depends_on:
      postgres:
        condition: service_healthy
      quartermaster:
        condition: service_healthy
      foghorn-redis:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - frameworks
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test:
        ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:18018/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Foghorn HA state store
  foghorn-redis:
    image: redis:7-alpine
    container_name: frameworks-foghorn-redis
    hostname: foghorn-redis
    command: redis-server --appendonly yes
    volumes:
      - foghorn_redis_data:/data
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - frameworks

  # Add Kafka infrastructure for event-driven analytics
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: frameworks-zookeeper
    hostname: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      # Reduce ZooKeeper log noise.
      ZOOKEEPER_LOG4J_ROOT_LOGLEVEL: WARN
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_logs:/var/lib/zookeeper/log
    networks:
      - frameworks
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: echo stat | nc localhost 2181 || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: frameworks-kafka
    hostname: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      # Reduce Kafka startup log spam (partition/leader state chatter).
      KAFKA_LOG4J_ROOT_LOGLEVEL: WARN
      KAFKA_LOG4J_LOGGERS: "kafka.controller=ERROR,kafka.coordinator.group=ERROR,kafka.cluster=ERROR,state.change.logger=ERROR,kafka.log=ERROR"
    volumes:
      - kafka_data:/var/lib/kafka/data
    ports:
      - "29092:29092"
    networks:
      - frameworks
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    mem_limit: 1g
    mem_reservation: 512m
    cpus: 2.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: kafka-topics --bootstrap-server localhost:9092 --list || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s

  # Kafka Topic Initialization
  kafka-init:
    image: confluentinc/cp-kafka:7.4.0
    container_name: frameworks-kafka-init
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      KAFKA_LOG4J_ROOT_LOGLEVEL: WARN
    entrypoint: ["/bin/sh", "-c"]
    command: |
      "
      echo 'Creating Kafka topics...'
      kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic ${ANALYTICS_KAFKA_TOPIC} --partitions 3 --replication-factor 1 --config retention.ms=604800000
      kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic ${SERVICE_EVENTS_KAFKA_TOPIC} --partitions 3 --replication-factor 1 --config retention.ms=604800000
      kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic ${BILLING_KAFKA_TOPIC} --partitions 3 --replication-factor 1 --config retention.ms=2592000000
      kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic ${DECKLOG_DLQ_KAFKA_TOPIC} --partitions 3 --replication-factor 1 --config retention.ms=2592000000
      echo 'Topics created successfully'
      "
    networks:
      - frameworks

  # One-shot migration runner to apply SQL migrations
  # db-migrate:
  #   image: postgres:15
  #   container_name: frameworks-db-migrate
  #   depends_on:
  #     postgres:
  #       condition: service_healthy
  #   entrypoint: [ '/bin/sh', '-c' ]
  #   command: |
  #     "
  #     set -e
  #     echo 'Applying SQL migrations...'
  #     for f in /migrations/*.sql; do
  #       echo "Running migration: $f"
  #       psql postgresql://frameworks_user:frameworks_dev@postgres:5432/frameworks -v ON_ERROR_STOP=1 -f "$f"
  #     done
  #     echo 'Migrations complete.'
  #     "
  #   volumes:
  #     - ./database/migrations:/migrations:ro
  #   networks:
  #     - frameworks
  #   restart: 'no'

  # Quartermaster - Tenant Management Service
  quartermaster:
    <<: *default-env
    container_name: frameworks-quartermaster
    build:
      context: .
      dockerfile: api_tenants/Dockerfile
    ports:
      - "18002:18002" # HTTP (health)
      - "19002:19002" # gRPC (internal)
    environment:
      DATABASE_URL: ${DATABASE_URL}
      PORT: ${QUARTERMASTER_PORT}
      GRPC_PORT: ${QUARTERMASTER_GRPC_PORT}
      LOG_LEVEL: ${LOG_LEVEL}
      JWT_SECRET: ${JWT_SECRET}
      SERVICE_TOKEN: ${SERVICE_TOKEN}
      CLUSTER_ID: ${CLUSTER_ID}
    depends_on:
      postgres:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - frameworks
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test:
        ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:18002/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Event Ingest (Decklog)
  decklog:
    <<: *default-env
    container_name: frameworks-decklog
    build:
      context: .
      dockerfile: api_firehose/Dockerfile
    hostname: decklog
    environment:
      KAFKA_BROKERS: ${KAFKA_BROKERS}
      KAFKA_CLUSTER_ID: ${KAFKA_CLUSTER_ID}
      ANALYTICS_KAFKA_TOPIC: ${ANALYTICS_KAFKA_TOPIC}
      REGION: ${REGION}
      PORT: ${DECKLOG_PORT}
      LOG_LEVEL: ${LOG_LEVEL}
      GIN_MODE: ${GIN_MODE}
      DECKLOG_ALLOW_INSECURE: ${DECKLOG_ALLOW_INSECURE}
      DECKLOG_TLS_CERT_FILE: ${DECKLOG_TLS_CERT_FILE}
      DECKLOG_TLS_KEY_FILE: ${DECKLOG_TLS_KEY_FILE}
      DECKLOG_METRICS_PORT: ${DECKLOG_METRICS_PORT}
      QUARTERMASTER_URL: ${QUARTERMASTER_URL}
      SERVICE_TOKEN: ${SERVICE_TOKEN}
      CLUSTER_ID: ${CLUSTER_ID}
    ports:
      - "18006:18006"
      - "18026:18026"
    depends_on:
      kafka:
        condition: service_healthy
      kafka-init:
        condition: service_completed_successfully
    restart: unless-stopped
    networks:
      - frameworks
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "grpcurl", "-plaintext", "localhost:18006", "grpc.health.v1.Health/Check"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 15s

  # Signalman - WebSocket Hub Service
  signalman:
    <<: *default-env
    container_name: frameworks-signalman
    build:
      context: .
      dockerfile: api_realtime/Dockerfile
    hostname: signalman
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy
      quartermaster:
        condition: service_healthy
    environment:
      PORT: ${SIGNALMAN_PORT}
      GRPC_PORT: ${SIGNALMAN_GRPC_PORT}
      DATABASE_URL: ${DATABASE_URL}
      KAFKA_BROKERS: ${KAFKA_BROKERS}
      ANALYTICS_KAFKA_TOPIC: ${ANALYTICS_KAFKA_TOPIC}
      KAFKA_GROUP_ID: ${SIGNALMAN_KAFKA_GROUP_ID}
      KAFKA_CLUSTER_ID: ${KAFKA_CLUSTER_ID}
      KAFKA_CLIENT_ID: ${SIGNALMAN_KAFKA_CLIENT_ID}
      LOG_LEVEL: ${LOG_LEVEL}
      REGION: ${REGION}
      JWT_SECRET: ${JWT_SECRET}
      SERVICE_TOKEN: ${SERVICE_TOKEN}
      CLUSTER_ID: ${CLUSTER_ID}
    ports:
      - "18009:18009"
      - "19005:19005"
    networks:
      - frameworks
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test:
        ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:18009/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Backend API
  commodore:
    <<: *default-env
    container_name: frameworks-commodore
    build:
      context: .
      dockerfile: api_control/Dockerfile
    environment:
      DATABASE_URL: ${DATABASE_URL}
      JWT_SECRET: ${JWT_SECRET}
      PASSWORD_RESET_SECRET: ${PASSWORD_RESET_SECRET}
      PORT: ${COMMODORE_PORT}
      GRPC_PORT: ${COMMODORE_GRPC_PORT}
      # gRPC addresses (internal)
      FOGHORN_CONTROL_ADDR: ${FOGHORN_CONTROL_ADDR}
      QUARTERMASTER_GRPC_ADDR: ${QUARTERMASTER_GRPC_ADDR}
      PURSER_GRPC_ADDR: ${PURSER_GRPC_ADDR}
      WEBAPP_PUBLIC_URL: ${WEBAPP_PUBLIC_URL}
      SERVICE_TOKEN: ${SERVICE_TOKEN}
      TURNSTILE_AUTH_SECRET_KEY: ${TURNSTILE_AUTH_SECRET_KEY}
      CLUSTER_ID: ${CLUSTER_ID}
    ports:
      - "18001:18001" # HTTP (auth, webhooks, health)
      - "19001:19001" # gRPC (internal)
    depends_on:
      postgres:
        condition: service_healthy
      quartermaster:
        condition: service_healthy
    networks:
      - frameworks
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test:
        ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:18001/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Edge Sidecar API
  helmsman:
    <<: *default-env
    container_name: frameworks-helmsman
    build:
      context: .
      dockerfile: api_sidecar/Dockerfile
    environment:
      HELMSMAN_WEBHOOK_URL: ${HELMSMAN_WEBHOOK_URL}
      FOGHORN_URL: ${FOGHORN_URL}
      FOGHORN_CONTROL_ADDR: ${FOGHORN_CONTROL_ADDR}
      EDGE_PUBLIC_URL: ${EDGE_PUBLIC_URL}
      MISTSERVER_URL: ${MISTSERVER_URL}
      MIST_PASSWORD: ${MIST_PASSWORD}
      MIST_API_USERNAME: ${MIST_API_USERNAME}
      MIST_API_PASSWORD: ${MIST_API_PASSWORD}
      NODE_ID: ${HELMSMAN_NODE_ID}
      PORT: ${HELMSMAN_PORT}
      LOG_LEVEL: ${LOG_LEVEL}
      SERVICE_TOKEN: ${SERVICE_TOKEN}
      GRPC_TLS_CERT_PATH: ${HELMSMAN_GRPC_TLS_CERT_PATH}
      GRPC_TLS_KEY_PATH: ${HELMSMAN_GRPC_TLS_KEY_PATH}
      HELMSMAN_STORAGE_LOCAL_PATH: /var/lib/mistserver/recordings
    volumes:
      - ./infrastructure/edge/machine-id:/etc/machine-id:ro
      - ./infrastructure/demo-recordings:/var/lib/mistserver/recordings
    depends_on:
      commodore:
        condition: service_started
      foghorn:
        condition: service_started
      mistserver:
        condition: service_started
      decklog:
        condition: service_started
    ports:
      - "18007:18007"
    networks:
      - frameworks
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test:
        ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:18007/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Periscope-Ingest: High-throughput event processing from Kafka
  periscope-ingest:
    <<: *default-env
    container_name: frameworks-periscope-ingest
    build:
      context: .
      dockerfile: api_analytics_ingest/Dockerfile
    ports:
      - "18005:18005"
    environment:
      DATABASE_URL: ${DATABASE_URL}
      KAFKA_BROKERS: ${KAFKA_BROKERS}
      KAFKA_GROUP_ID: ${PERISCOPE_INGEST_KAFKA_GROUP_ID}
      KAFKA_CLUSTER_ID: ${KAFKA_CLUSTER_ID}
      ANALYTICS_KAFKA_TOPIC: ${ANALYTICS_KAFKA_TOPIC}
      KAFKA_CLIENT_ID: ${PERISCOPE_INGEST_KAFKA_CLIENT_ID}
      LOG_LEVEL: ${LOG_LEVEL}
      ENABLE_HEALTH_ENDPOINT: ${PERISCOPE_INGEST_ENABLE_HEALTH_ENDPOINT}
      HEALTH_PORT: ${PERISCOPE_INGEST_PORT}
      CLICKHOUSE_HOST: ${CLICKHOUSE_HOST}
      CLICKHOUSE_PORT: ${CLICKHOUSE_PORT}
      CLICKHOUSE_DB: ${CLICKHOUSE_DB}
      CLICKHOUSE_USER: ${CLICKHOUSE_USER}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD}
      QUARTERMASTER_URL: ${QUARTERMASTER_URL}
      SERVICE_TOKEN: ${SERVICE_TOKEN}
      CLUSTER_ID: ${CLUSTER_ID}
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy
      kafka-init:
        condition: service_completed_successfully
      clickhouse:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - frameworks
    mem_limit: 1g
    mem_reservation: 512m
    cpus: 2.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test:
        ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:18005/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 15s

  # Periscope-Query (Analytics API - Query Side)
  periscope-query:
    <<: *default-env
    container_name: frameworks-periscope-query
    build:
      context: .
      dockerfile: api_analytics_query/Dockerfile
    ports:
      - "18004:18004"
      - "19004:19004"
    environment:
      DATABASE_URL: ${DATABASE_URL}
      KAFKA_BROKERS: ${KAFKA_BROKERS}
      BILLING_KAFKA_TOPIC: ${BILLING_KAFKA_TOPIC}
      PORT: ${PERISCOPE_QUERY_PORT}
      GRPC_PORT: ${PERISCOPE_QUERY_GRPC_PORT}
      LOG_LEVEL: ${LOG_LEVEL}
      GIN_MODE: ${GIN_MODE}
      JWT_SECRET: ${JWT_SECRET}
      SERVICE_TOKEN: ${SERVICE_TOKEN}
      QUARTERMASTER_GRPC_ADDR: ${QUARTERMASTER_GRPC_ADDR}
      CLICKHOUSE_HOST: ${CLICKHOUSE_HOST}
      CLICKHOUSE_PORT: ${CLICKHOUSE_PORT}
      CLICKHOUSE_DB: ${CLICKHOUSE_DB}
      CLICKHOUSE_USER: ${CLICKHOUSE_USER}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD}
      CLUSTER_ID: ${CLUSTER_ID}
    depends_on:
      postgres:
        condition: service_healthy
      purser:
        condition: service_started
      clickhouse:
        condition: service_healthy
      quartermaster:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - frameworks
    mem_limit: 1g
    mem_reservation: 512m
    cpus: 2.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test:
        ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:18004/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Purser (Billing API)
  purser:
    <<: *default-env
    container_name: frameworks-purser
    build:
      context: .
      dockerfile: api_billing/Dockerfile
    ports:
      - "18003:18003" # HTTP (webhooks, health)
      - "19003:19003" # gRPC (internal)
    environment:
      DATABASE_URL: ${DATABASE_URL}
      KAFKA_BROKERS: ${KAFKA_BROKERS}
      BILLING_KAFKA_TOPIC: ${BILLING_KAFKA_TOPIC}
      KAFKA_GROUP_ID: ${PURSER_KAFKA_GROUP_ID}
      KAFKA_CLIENT_ID: ${PURSER_KAFKA_CLIENT_ID}
      PORT: ${PURSER_PORT}
      GRPC_PORT: ${PURSER_GRPC_PORT}
      LOG_LEVEL: ${LOG_LEVEL}
      STRIPE_SECRET_KEY: ${STRIPE_SECRET_KEY}
      STRIPE_WEBHOOK_SECRET: ${STRIPE_WEBHOOK_SECRET}
      MOLLIE_API_KEY: ${MOLLIE_API_KEY}
      MOLLIE_WEBHOOK_SECRET: ${MOLLIE_WEBHOOK_SECRET}
      GIN_MODE: ${GIN_MODE}
      JWT_SECRET: ${JWT_SECRET}
      SERVICE_TOKEN: ${SERVICE_TOKEN}
      QUARTERMASTER_GRPC_ADDR: ${QUARTERMASTER_GRPC_ADDR}
      COMMODORE_GRPC_ADDR: ${COMMODORE_GRPC_ADDR}
      DECKLOG_GRPC_ADDR: ${DECKLOG_GRPC_ADDR}
      PERISCOPE_GRPC_ADDR: ${PERISCOPE_GRPC_ADDR}
      PURSER_HOST: ${PURSER_HOST}
      CLUSTER_ID: ${CLUSTER_ID}
      KAFKA_CLUSTER_ID: ${KAFKA_CLUSTER_ID}
      API_PUBLIC_URL: ${API_PUBLIC_URL}
      WEBAPP_PUBLIC_URL: ${WEBAPP_PUBLIC_URL}
      GATEWAY_PUBLIC_URL: ${GATEWAY_PUBLIC_URL}
      # Crypto + x402 (optional)
      HD_WALLET_XPUB: ${HD_WALLET_XPUB}
      ETHERSCAN_API_KEY: ${ETHERSCAN_API_KEY}
      BASESCAN_API_KEY: ${BASESCAN_API_KEY}
      ARBISCAN_API_KEY: ${ARBISCAN_API_KEY}
      CRYPTO_INCLUDE_TESTNETS: ${CRYPTO_INCLUDE_TESTNETS}
      X402_GAS_WALLET_PRIVKEY: ${X402_GAS_WALLET_PRIVKEY}
      X402_GAS_WALLET_ADDRESS: ${X402_GAS_WALLET_ADDRESS}
      X402_INCLUDE_TESTNETS: ${X402_INCLUDE_TESTNETS}
      ETH_RPC_ENDPOINT: ${ETH_RPC_ENDPOINT}
      BASE_RPC_ENDPOINT: ${BASE_RPC_ENDPOINT}
      ARBITRUM_RPC_ENDPOINT: ${ARBITRUM_RPC_ENDPOINT}
      BASE_SEPOLIA_RPC_ENDPOINT: ${BASE_SEPOLIA_RPC_ENDPOINT}
      ARBITRUM_SEPOLIA_RPC_ENDPOINT: ${ARBITRUM_SEPOLIA_RPC_ENDPOINT}
      # Email/SMTP configuration (optional)
      SMTP_HOST: ${SMTP_HOST}
      SMTP_PORT: ${SMTP_PORT}
      SMTP_USER: ${SMTP_USER}
      SMTP_PASSWORD: ${SMTP_PASSWORD}
      FROM_EMAIL: ${FROM_EMAIL}
      FROM_NAME: ${FROM_NAME}
    depends_on:
      postgres:
        condition: service_healthy
      quartermaster:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - frameworks
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test:
        ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:18003/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Skipper (AI Video Consultant API)
  skipper:
    <<: *default-env
    container_name: frameworks-skipper
    build:
      context: .
      dockerfile: api_consultant/Dockerfile
    ports:
      - "18018:18018"
      - "19007:19007"
    environment:
      DATABASE_URL: ${DATABASE_URL}
      PORT: ${SKIPPER_PORT}
      GRPC_PORT: ${SKIPPER_GRPC_PORT}
      LOG_LEVEL: ${LOG_LEVEL}
      GIN_MODE: ${GIN_MODE}
      JWT_SECRET: ${JWT_SECRET}
      SERVICE_TOKEN: ${SERVICE_TOKEN}
      COMMODORE_GRPC_ADDR: ${COMMODORE_GRPC_ADDR}
      PERISCOPE_GRPC_ADDR: ${PERISCOPE_GRPC_ADDR}
      QUARTERMASTER_GRPC_ADDR: ${QUARTERMASTER_GRPC_ADDR}
      SKIPPER_HOST: ${SKIPPER_HOST:-skipper}
      CLUSTER_ID: ${CLUSTER_ID}
      LLM_PROVIDER: ${LLM_PROVIDER}
      LLM_MODEL: ${LLM_MODEL}
      LLM_API_KEY: ${LLM_API_KEY}
      LLM_API_URL: ${LLM_API_URL}
      EMBEDDING_PROVIDER: ${EMBEDDING_PROVIDER:-}
      EMBEDDING_MODEL: ${EMBEDDING_MODEL:-}
      EMBEDDING_API_KEY: ${EMBEDDING_API_KEY:-}
      EMBEDDING_API_URL: ${EMBEDDING_API_URL:-}
      SKIPPER_WEB_UI: ${SKIPPER_WEB_UI:-true}
      SKIPPER_API_KEY: ${SKIPPER_API_KEY:-}
      SEARCH_PROVIDER: ${SEARCH_PROVIDER}
      SEARCH_API_KEY: ${SEARCH_API_KEY}
      SEARCH_API_URL: ${SEARCH_API_URL}
      GATEWAY_PUBLIC_URL: http://${BRIDGE_HOST}:${BRIDGE_PORT}
      DOCS_PUBLIC_URL: ${DOCS_PUBLIC_URL}
      MARKETING_PUBLIC_URL: ${MARKETING_PUBLIC_URL}
      SITEMAPS: ${SITEMAPS:-}
      SKIPPER_SITEMAPS_DIR: /etc/skipper/sitemaps
      CRAWL_INTERVAL: ${CRAWL_INTERVAL:-24h}
    volumes:
      - ./config/skipper/sitemaps:/etc/skipper/sitemaps:ro
    depends_on:
      postgres:
        condition: service_healthy
      quartermaster:
        condition: service_healthy
      commodore:
        condition: service_started
      periscope-query:
        condition: service_started
      bridge:
        condition: service_started
    networks:
      - frameworks
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test:
        ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:18018/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Ollama (optional local LLM runtime)
  # Start with: docker compose --profile llm up
  ollama:
    image: ollama/ollama:latest
    container_name: frameworks-ollama
    profiles:
      - llm
    ports:
      - "11434:11434"
    networks:
      - frameworks
    mem_limit: 2g
    mem_reservation: 1g
    cpus: 2.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # Steward (Forms service)
  steward:
    <<: *default-env
    container_name: frameworks-steward
    build:
      context: .
      dockerfile: api_forms/Dockerfile
    ports:
      - "18032:18032"
    environment:
      PORT: ${API_FORMS_PORT}
      NODE_ENV: ${NODE_ENV}
      FORMS_ALLOWED_ORIGINS: ${FORMS_ALLOWED_ORIGINS}
      TURNSTILE_FORMS_SECRET_KEY: ${TURNSTILE_FORMS_SECRET_KEY}
      SMTP_HOST: ${SMTP_HOST}
      SMTP_PORT: ${SMTP_PORT}
      SMTP_USER: ${SMTP_USER}
      SMTP_PASSWORD: ${SMTP_PASSWORD}
      FROM_EMAIL: ${FROM_EMAIL}
      TO_EMAIL: ${TO_EMAIL}
      # Listmonk Integration
      LISTMONK_URL: http://listmonk:9000
      LISTMONK_USERNAME: ${LISTMONK_USERNAME:-admin}
      LISTMONK_PASSWORD: ${LISTMONK_PASSWORD:-admin}
      DEFAULT_MAILING_LIST_ID: ${DEFAULT_MAILING_LIST_ID:-1}
    restart: unless-stopped
    networks:
      - frameworks
    mem_limit: 256m
    mem_reservation: 128m
    cpus: 0.5
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test:
        ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:18032/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Chartroom (Web Console)
  chartroom:
    container_name: frameworks-chartroom
    build:
      context: .
      dockerfile: website_application/Dockerfile
      args:
        - BUILD_ENV=${BUILD_ENV}
        - APP_PORT=${WEBAPP_PORT}
        - VITE_AUTH_URL=${VITE_AUTH_URL}
        - VITE_GRAPHQL_HTTP_URL=${VITE_GRAPHQL_HTTP_URL}
        - VITE_GRAPHQL_WS_URL=${VITE_GRAPHQL_WS_URL}
        - VITE_STREAMING_INGEST_URL=${VITE_STREAMING_INGEST_URL}
        - VITE_STREAMING_PLAY_URL=${VITE_STREAMING_PLAY_URL}
        - VITE_STREAMING_EDGE_URL=${VITE_STREAMING_EDGE_URL}
        - VITE_STREAMING_RTMP_PORT=${VITE_STREAMING_RTMP_PORT}
        - VITE_STREAMING_SRT_PORT=${VITE_STREAMING_SRT_PORT}
        - VITE_STREAMING_RTMP_PATH=${VITE_STREAMING_RTMP_PATH}
        - VITE_STREAMING_HLS_PATH=${VITE_STREAMING_HLS_PATH}
        - VITE_STREAMING_WEBRTC_PATH=${VITE_STREAMING_WEBRTC_PATH}
        - VITE_STREAMING_EMBED_PATH=${VITE_STREAMING_EMBED_PATH}
        - VITE_MARKETING_SITE_URL=${VITE_MARKETING_SITE_URL}
        - VITE_DOCS_SITE_URL=${VITE_DOCS_SITE_URL}
        - VITE_TURNSTILE_AUTH_SITE_KEY=${VITE_TURNSTILE_AUTH_SITE_KEY}
        - BASE_PATH=${BASE_PATH}
        - VITE_GATEWAY_URL=${VITE_GATEWAY_URL}
        - VITE_GITHUB_URL=${VITE_GITHUB_URL}
    depends_on:
      commodore:
        condition: service_started
    environment:
      - NODE_ENV=${NODE_ENV}
      - PORT=${WEBAPP_PORT}
      - HOST=0.0.0.0
    ports:
      - "${WEBAPP_PORT:-18030}:${WEBAPP_PORT:-18030}"
    networks:
      - frameworks
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # Foredeck (Marketing Website)
  foredeck:
    container_name: frameworks-foredeck
    build:
      context: .
      dockerfile: website_marketing/Dockerfile
      args:
        - BUILD_ENV=${BUILD_ENV}
        - APP_PORT=${WEBSITE_PORT}
        - VITE_APP_URL=${VITE_APP_URL}
        - VITE_GATEWAY_URL=${VITE_GATEWAY_URL}
        - VITE_GITHUB_URL=${VITE_GITHUB_URL}
        - VITE_LIVEPEER_URL=${VITE_LIVEPEER_URL}
        - VITE_LIVEPEER_EXPLORER_URL=${VITE_LIVEPEER_EXPLORER_URL}
        - VITE_CONTACT_EMAIL=${VITE_CONTACT_EMAIL}
        - VITE_FORUM_URL=${VITE_FORUM_URL}
        - VITE_DISCORD_URL=${VITE_DISCORD_URL}
        - VITE_DEMO_STREAM_NAME=${VITE_DEMO_STREAM_NAME}
        - VITE_COMPANY_NAME=${VITE_COMPANY_NAME}
        - VITE_TURNSTILE_FORMS_SITE_KEY=${VITE_TURNSTILE_FORMS_SITE_KEY}
        - VITE_DOMAIN=${VITE_DOMAIN}
        - VITE_CONTACT_API_URL=${VITE_CONTACT_API_URL}
        - VITE_DOCS_SITE_URL=${VITE_DOCS_SITE_URL}
    hostname: foredeck
    environment:
      - NODE_ENV=${NODE_ENV}
      - PORT=${WEBSITE_PORT}
      - HOST=0.0.0.0
    ports:
      - "${WEBSITE_PORT:-18031}:${WEBSITE_PORT:-18031}"
    depends_on:
      bridge:
        condition: service_started
    restart: unless-stopped
    networks:
      - frameworks
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test:
        ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:18031/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Logbook (Documentation Website)
  logbook:
    container_name: frameworks-logbook
    build:
      context: .
      dockerfile: website_docs/Dockerfile
      args:
        - BUILD_ENV=${BUILD_ENV}
        - APP_PORT=${WEBSITE_DOCS_PORT}
        - PUBLIC_DOCS_URL=${DOCS_PUBLIC_URL}
        - WEBAPP_PUBLIC_URL=${WEBAPP_PUBLIC_URL}
        - MARKETING_PUBLIC_URL=${MARKETING_PUBLIC_URL}
        - GATEWAY_PUBLIC_URL=${GATEWAY_PUBLIC_URL}
        - PUBLIC_TURNSTILE_AUTH_SITE_KEY=${TURNSTILE_AUTH_SITE_KEY}
        - STREAMING_INGEST_URL=${STREAMING_INGEST_URL}
        - STREAMING_EDGE_URL=${STREAMING_EDGE_URL}
        - STREAMING_PLAY_URL=${STREAMING_PLAY_URL}
        - STREAMING_RTMP_PORT=${STREAMING_RTMP_PORT}
        - STREAMING_SRT_PORT=${STREAMING_SRT_PORT}
        - STREAMING_RTMP_PATH=${STREAMING_RTMP_PATH}
        - STREAMING_HLS_PATH=${STREAMING_HLS_PATH}
        - STREAMING_WEBRTC_PATH=${STREAMING_WEBRTC_PATH}
        - DISCORD_URL=${DISCORD_URL}
        - GITHUB_URL=${GITHUB_URL}
    hostname: logbook
    environment:
      - NODE_ENV=${NODE_ENV}
      - PORT=${WEBSITE_DOCS_PORT}
      - HOST=0.0.0.0
    ports:
      - "${WEBSITE_DOCS_PORT:-18033}:${WEBSITE_DOCS_PORT:-18033}"
    networks:
      - frameworks
    mem_limit: 256m
    mem_reservation: 128m
    cpus: 0.5
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://localhost:${WEBSITE_DOCS_PORT:-18033}/",
        ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Reverse Proxy
  nginx:
    image: nginx:alpine
    container_name: frameworks-nginx
    ports:
      - "18090:80"
    volumes:
      - ./infrastructure/nginx/default.conf:/etc/nginx/conf.d/default.conf:ro
    depends_on:
      bridge:
        condition: service_started
      mistserver:
        condition: service_started
      foghorn:
        condition: service_started
      foghorn-2:
        condition: service_started
      commodore:
        condition: service_started
      helmsman:
        condition: service_started
      periscope-query:
        condition: service_started
      chartroom:
        condition: service_started
      foredeck:
        condition: service_started
      quartermaster:
        condition: service_healthy
    networks:
      - frameworks
    mem_limit: 256m
    mem_reservation: 128m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:80/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s

  # ClickHouse for time-series analytics
  clickhouse:
    image: clickhouse/clickhouse-server:latest
    container_name: frameworks-clickhouse
    hostname: clickhouse
    ports:
      - "8123:8123" # HTTP interface
      - "9000:9000" # Native interface
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ./infrastructure/clickhouse/users.xml:/etc/clickhouse-server/users.xml:ro
      - ./infrastructure/clickhouse/config.xml:/etc/clickhouse-server/config.d/frameworks.xml:ro
      # Schema first, then demo seed data (ordering by filename)
      - ./pkg/database/sql/clickhouse/periscope.sql:/docker-entrypoint-initdb.d/01_init.sql:ro
      - ./pkg/database/sql/seeds/demo/clickhouse_demo_data.sql:/docker-entrypoint-initdb.d/02_seed_demo.sql:ro
    environment:
      # Start with default user for initialization
      CLICKHOUSE_USER: default
      CLICKHOUSE_DB: periscope
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
      # Environment variables for users.xml
      CLICKHOUSE_PASSWORD: frameworks_dev # For frameworks user
      CLICKHOUSE_READONLY_PASSWORD: readonly_dev
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    mem_limit: 2g
    mem_reservation: 1g
    cpus: 2.0
    networks:
      - frameworks
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8123/ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Listmonk Newsletter Manager
  listmonk:
    image: listmonk/listmonk:v4.0.1
    container_name: frameworks-listmonk
    hostname: listmonk
    ports:
      - "9001:9000"
    entrypoint: ["sh", "-c", "./listmonk --install --idempotent --yes && ./listmonk"]
    environment:
      LISTMONK_app__address: "0.0.0.0:9000"
      LISTMONK_app__admin_username: ${LISTMONK_USERNAME:-admin}
      LISTMONK_app__admin_password: ${LISTMONK_PASSWORD:-admin}
      LISTMONK_db__host: postgres
      LISTMONK_db__port: 5432
      LISTMONK_db__user: listmonk
      LISTMONK_db__password: listmonk_dev
      LISTMONK_db__database: listmonk
      LISTMONK_db__ssl_mode: disable
      # SMTP Configuration
      LISTMONK_app__smtp__host: ${SMTP_HOST}
      LISTMONK_app__smtp__port: ${SMTP_PORT}
      LISTMONK_app__smtp__username: ${SMTP_USER}
      LISTMONK_app__smtp__password: ${SMTP_PASSWORD}
      LISTMONK_app__smtp__auth_protocol: "login"
      LISTMONK_app__smtp__tls_type: "STARTTLS"
      LISTMONK_app__from_email: ${FROM_EMAIL}
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - frameworks
    restart: unless-stopped

  # ============================================================================
  # CHATWOOT - Customer Support Chat (Self-hosted)
  # Provides live chat widget for webapp, agent dashboard for you
  # ============================================================================

  chatwoot:
    <<: *default-env
    image: chatwoot/chatwoot:v3.14.0
    container_name: frameworks-chatwoot
    hostname: chatwoot
    # Run migrations on startup, then start the server
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        bundle exec rails db:chatwoot_prepare
        bundle exec rails s -p 3000 -b '0.0.0.0'
    environment:
      # Core
      SECRET_KEY_BASE: ${CHATWOOT_SECRET_KEY}
      FRONTEND_URL: ${CHATWOOT_FRONTEND_URL:-http://localhost:18092}
      # Database
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DATABASE: chatwoot
      POSTGRES_USERNAME: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      # Redis (platform-wide: Chatwoot + Deckhand)
      REDIS_URL: redis://platform-redis:6379
      # SMTP for notifications
      SMTP_ADDRESS: ${SMTP_HOST}
      SMTP_PORT: ${SMTP_PORT}
      SMTP_USERNAME: ${SMTP_USER}
      SMTP_PASSWORD: ${SMTP_PASSWORD}
      SMTP_DOMAIN: ${CHATWOOT_SMTP_DOMAIN:-frameworks.com}
      SMTP_AUTHENTICATION: login
      SMTP_ENABLE_STARTTLS_AUTO: "true"
      MAILER_SENDER_EMAIL: ${CHATWOOT_MAILER_EMAIL:-support@frameworks.com}
      # Rails
      RAILS_ENV: production
      RAILS_LOG_TO_STDOUT: "true"
      LOG_LEVEL: info
    ports:
      - "18092:3000"
    depends_on:
      postgres:
        condition: service_healthy
      platform-redis:
        condition: service_healthy
    networks:
      - frameworks
    mem_limit: 1g
    mem_reservation: 512m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://127.0.0.1:3000/api"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  chatwoot-worker:
    <<: *default-env
    image: chatwoot/chatwoot:v3.14.0
    container_name: frameworks-chatwoot-worker
    hostname: chatwoot-worker
    command: bundle exec sidekiq -C config/sidekiq.yml
    environment:
      # Same as chatwoot main
      SECRET_KEY_BASE: ${CHATWOOT_SECRET_KEY}
      FRONTEND_URL: ${CHATWOOT_FRONTEND_URL:-http://localhost:18092}
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DATABASE: chatwoot
      POSTGRES_USERNAME: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      REDIS_URL: redis://platform-redis:6379
      SMTP_ADDRESS: ${SMTP_HOST}
      SMTP_PORT: ${SMTP_PORT}
      SMTP_USERNAME: ${SMTP_USER}
      SMTP_PASSWORD: ${SMTP_PASSWORD}
      SMTP_DOMAIN: ${CHATWOOT_SMTP_DOMAIN:-frameworks.com}
      SMTP_AUTHENTICATION: login
      SMTP_ENABLE_STARTTLS_AUTO: "true"
      MAILER_SENDER_EMAIL: ${CHATWOOT_MAILER_EMAIL:-support@frameworks.com}
      RAILS_ENV: production
      RAILS_LOG_TO_STDOUT: "true"
      LOG_LEVEL: info
    depends_on:
      chatwoot:
        condition: service_healthy
      platform-redis:
        condition: service_healthy
    networks:
      - frameworks
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 0.5
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  platform-redis:
    image: redis:7-alpine
    container_name: frameworks-platform-redis
    hostname: platform-redis
    command: redis-server --appendonly yes
    volumes:
      - platform_redis_data:/data
    networks:
      - frameworks
    mem_limit: 256m
    mem_reservation: 128m
    cpus: 0.25
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s

  # Deckhand - Support Enrichment Service
  # Receives Chatwoot webhooks, enriches with tenant/billing context
  deckhand:
    <<: *default-env
    container_name: frameworks-deckhand
    build:
      context: .
      dockerfile: api_ticketing/Dockerfile
    environment:
      DECKHAND_PORT: ${DECKHAND_PORT:-18015}
      DECKHAND_GRPC_PORT: ${DECKHAND_GRPC_PORT:-19006}
      DECKHAND_WEBHOOK_RATE_LIMIT_PER_MIN: ${DECKHAND_WEBHOOK_RATE_LIMIT_PER_MIN:-600}
      LOG_LEVEL: ${LOG_LEVEL}
      GIN_MODE: ${GIN_MODE}
      # Chatwoot integration
      CHATWOOT_HOST: ${CHATWOOT_HOST:-chatwoot}
      CHATWOOT_PORT: ${CHATWOOT_PORT:-3000}
      CHATWOOT_ACCOUNT_ID: ${CHATWOOT_ACCOUNT_ID:-1}
      CHATWOOT_INBOX_ID: ${CHATWOOT_INBOX_ID:-1}
      CHATWOOT_API_TOKEN: ${CHATWOOT_API_TOKEN}
      # gRPC addresses for enrichment
      QUARTERMASTER_GRPC_ADDR: ${QUARTERMASTER_GRPC_ADDR:-quartermaster:19002}
      PURSER_GRPC_ADDR: ${PURSER_GRPC_ADDR:-purser:19003}
      DECKLOG_GRPC_ADDR: ${DECKLOG_GRPC_ADDR:-decklog:18006}
      JWT_SECRET: ${JWT_SECRET}
      SERVICE_TOKEN: ${SERVICE_TOKEN}
      REDIS_ADDR: ${REDIS_ADDR:-}
    depends_on:
      quartermaster:
        condition: service_healthy
      purser:
        condition: service_started
      chatwoot:
        condition: service_healthy
    networks:
      - frameworks
    mem_limit: 256m
    mem_reservation: 128m
    cpus: 0.5
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test:
        ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:18015/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
