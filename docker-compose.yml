# FrameWorks Development Environment
# Boots all the microservices, interfaces, media server, etc.

networks:
  frameworks:
    driver: bridge

volumes:
  postgres_data:
  mistserver_data:
  zookeeper_data:
  zookeeper_logs:
  kafka_data:
  clickhouse_data:
  prometheus_data:
  grafana_data:

x-default-env: &default-env
  env_file:
    - ${ENV_FILE:-.env}

services:
  # Bridge - GraphQL API Gateway
  bridge:
    <<: *default-env
    container_name: frameworks-bridge
    build:
      context: .
      dockerfile: api_gateway/Dockerfile
    environment:
      BRIDGE_PORT: ${BRIDGE_PORT}
      LOG_LEVEL: ${LOG_LEVEL}
      GIN_MODE: ${GIN_MODE}
      GRAPHQL_PLAYGROUND_ENABLED: ${GRAPHQL_PLAYGROUND_ENABLED}
      GRAPHQL_COMPLEXITY_LIMIT: ${GRAPHQL_COMPLEXITY_LIMIT}
      COMMODORE_URL: ${COMMODORE_URL}
      QUARTERMASTER_URL: ${QUARTERMASTER_URL}
      PERISCOPE_QUERY_URL: ${PERISCOPE_QUERY_URL}
      PURSER_URL: ${PURSER_URL}
      SIGNALMAN_WS_URL: ${SIGNALMAN_WS_URL}
      WS_MAX_CONNECTIONS_PER_TENANT: ${WS_MAX_CONNECTIONS_PER_TENANT}
      JWT_SECRET: ${JWT_SECRET}
      SERVICE_TOKEN: ${SERVICE_TOKEN}
      DECKLOG_URL: ${DECKLOG_URL}
      DECKLOG_ALLOW_INSECURE: ${DECKLOG_ALLOW_INSECURE}
      DECKLOG_USE_TLS: ${DECKLOG_USE_TLS}
      FOGHORN_URL: ${FOGHORN_URL}
      FOGHORN_CONTROL_ADDR: ${FOGHORN_CONTROL_ADDR}
      EDGE_PUBLIC_HOSTNAME: ${EDGE_PUBLIC_HOSTNAME}
      MISTSERVER_URL: ${MISTSERVER_URL}
    ports:
      - "18000:18000"
    depends_on:
      quartermaster:
        condition: service_healthy
      commodore:
        condition: service_started
      periscope-query:
        condition: service_started
      purser:
        condition: service_started
    networks:
      - frameworks
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:18000/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # PostgreSQL
  postgres:
    <<: *default-env
    image: postgres:15
    container_name: frameworks-postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/init_quartermaster.sql:/docker-entrypoint-initdb.d/10_init_quartermaster.sql:ro
      - ./database/init_purser.sql:/docker-entrypoint-initdb.d/20_init_purser.sql:ro
      - ./database/init_foghorn.sql:/docker-entrypoint-initdb.d/30_init_foghorn.sql:ro
      - ./database/init_commodore.sql:/docker-entrypoint-initdb.d/40_init_commodore.sql:ro
      - ./database/init_periscope.sql:/docker-entrypoint-initdb.d/50_init_periscope.sql:ro
      - ./database/init_demo.sql:/docker-entrypoint-initdb.d/99_init_demo.sql:ro
      # - ./database/migrations:/docker-entrypoint-initdb.d/migrations:ro
    ports:
      - "5432:5432"
    networks:
      - frameworks
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    mem_limit: 1g
    mem_reservation: 512m
    cpus: 2.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U frameworks_user -d frameworks"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # MistServer with AV libs.
  # Note: MistServer requires significant shared memory for holding stream buffers.
  mistserver:
    image: livepeerframeworks/mistserver:latest
    container_name: frameworks-mistserver
    platform: linux/amd64
    shm_size: '2gb'  # Allocate 2GB of shared memory for MistServer
    ports:
      - "4242:4242"       # Controller
      - "8080:8080"       # HTTP
      - "1935:1935"       # RTMP
      - "5554:5554"       # RTSP
      - "4200:4200"       # DTSC (Mist internal format; used for stream replication)
      - "8889:8889/udp"   # TS-SRT
      - "18203:18203/udp" # WebRTC
    volumes:
      # TODO: Make this read-only once initial configuration is stable
      # Currently writable as we're actively developing/testing configs
      - ./infrastructure/mistserver.conf:/etc/mistserver.conf
      - mistserver_data:/var/lib/mistserver
    restart: unless-stopped
    command: ["MistController", "-c", "/etc/mistserver.conf"]
    networks:
      - frameworks
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    mem_limit: 2g
    mem_reservation: 1g
    cpus: 2.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:4242/"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Foghorn Load Balancer (Regional/Central)
  foghorn:
    <<: *default-env
    # MistServer Load Balancer (comment out to use FrameWorks Foghorn)
    # image: ddvtech/mistserver_loadbalancer:latest
    # command: ["/usr/bin/MistUtilLoad", "-g", "5", "-P", "koekjes", "-p", "8042", "-s", "http://mistserver:4242"]

    # FrameWorks Foghorn
    container_name: frameworks-foghorn
    build:
      context: .
      dockerfile: api_balancing/Dockerfile
    environment:
      DATABASE_URL: ${DATABASE_URL}
      PORT: ${FOGHORN_PORT}
      FOGHORN_CONTROL_ADDR: ${FOGHORN_CONTROL_BIND_ADDR}
      LOG_LEVEL: ${LOG_LEVEL}
      GIN_MODE: ${GIN_MODE}
      COMMODORE_URL: ${COMMODORE_URL}
      COMMODORE_CACHE_TTL: ${COMMODORE_CACHE_TTL}
      COMMODORE_CACHE_SWR: ${COMMODORE_CACHE_SWR}
      COMMODORE_CACHE_NEG_TTL: ${COMMODORE_CACHE_NEG_TTL}
      COMMODORE_CACHE_MAX: ${COMMODORE_CACHE_MAX}
      GEOIP_MMDB_PATH: ${GEOIP_MMDB_PATH}
      GEOIP_CACHE_TTL: ${GEOIP_CACHE_TTL}
      GEOIP_CACHE_SWR: ${GEOIP_CACHE_SWR}
      GEOIP_CACHE_NEG_TTL: ${GEOIP_CACHE_NEG_TTL}
      GEOIP_CACHE_MAX: ${GEOIP_CACHE_MAX}
      DECKLOG_URL: ${DECKLOG_URL}
      DECKLOG_ALLOW_INSECURE: ${DECKLOG_ALLOW_INSECURE}  # For local development only
      # gRPC Security Configuration
      GRPC_USE_TLS: ${FOGHORN_GRPC_USE_TLS}  # Set to true for production TLS
      GRPC_TLS_CERT_PATH: ${FOGHORN_GRPC_TLS_CERT_PATH}  # Path to TLS certificate
      GRPC_TLS_KEY_PATH: ${FOGHORN_GRPC_TLS_KEY_PATH}    # Path to TLS private key
      DECKLOG_USE_TLS: ${DECKLOG_USE_TLS}  # Set to true for secure Decklog connections
      QUARTERMASTER_URL: ${QUARTERMASTER_URL}
      CPU_WEIGHT: ${CPU_WEIGHT}
      RAM_WEIGHT: ${RAM_WEIGHT}
      BANDWIDTH_WEIGHT: ${BANDWIDTH_WEIGHT}
      GEO_WEIGHT: ${GEO_WEIGHT}
      STREAM_BONUS: ${STREAM_BONUS}
      MISTSERVER_URL: ${MISTSERVER_URL}
      SERVICE_TOKEN: ${SERVICE_TOKEN}
    ports:
      - "18008:18008"
      - "18019:18019"
    depends_on:
      postgres:
        condition: service_healthy
      quartermaster:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - frameworks
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:18008/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Add Kafka infrastructure for event-driven analytics
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: frameworks-zookeeper
    hostname: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_logs:/var/lib/zookeeper/log
    networks:
      - frameworks
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: echo stat | nc localhost 2181 || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: frameworks-kafka
    hostname: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
    volumes:
      - kafka_data:/var/lib/kafka/data
    ports:
      - "29092:29092"
    networks:
      - frameworks
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    mem_limit: 1g
    mem_reservation: 512m
    cpus: 2.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: kafka-topics --bootstrap-server localhost:9092 --list || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s

  # Kafka Topic Initialization
  kafka-init:
    image: confluentinc/cp-kafka:7.4.0
    container_name: frameworks-kafka-init
    depends_on:
      kafka:
        condition: service_healthy
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      echo 'Creating Kafka topics...'
      kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic analytics_events --partitions 3 --replication-factor 1
      echo 'Topics created successfully'
      "
    networks:
      - frameworks

  # One-shot migration runner to apply SQL migrations
  # db-migrate:
  #   image: postgres:15
  #   container_name: frameworks-db-migrate
  #   depends_on:
  #     postgres:
  #       condition: service_healthy
  #   entrypoint: [ '/bin/sh', '-c' ]
  #   command: |
  #     "
  #     set -e
  #     echo 'Applying SQL migrations...'
  #     for f in /migrations/*.sql; do
  #       echo "Running migration: $f"
  #       psql postgresql://frameworks_user:frameworks_dev@postgres:5432/frameworks -v ON_ERROR_STOP=1 -f "$f"
  #     done
  #     echo 'Migrations complete.'
  #     "
  #   volumes:
  #     - ./database/migrations:/migrations:ro
  #   networks:
  #     - frameworks
  #   restart: 'no'

  # Quartermaster - Tenant Management Service
  quartermaster:
    <<: *default-env
    container_name: frameworks-quartermaster
    build:
      context: .
      dockerfile: api_tenants/Dockerfile
    ports:
      - "18002:18002"
    environment:
      DATABASE_URL: ${DATABASE_URL}
      PORT: ${QUARTERMASTER_PORT}
      LOG_LEVEL: ${LOG_LEVEL}
      JWT_SECRET: ${JWT_SECRET}
      SERVICE_TOKEN: ${SERVICE_TOKEN}
    depends_on:
      postgres:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - frameworks
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:18002/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Event Ingest (Decklog)
  decklog:
    <<: *default-env
    container_name: frameworks-decklog
    build:
      context: .
      dockerfile: api_firehose/Dockerfile
    hostname: decklog
    environment:
      KAFKA_BROKERS: ${KAFKA_BROKERS}
      KAFKA_CLUSTER_ID: ${KAFKA_CLUSTER_ID}
      REGION: ${REGION}
      PORT: ${DECKLOG_PORT}
      LOG_LEVEL: ${LOG_LEVEL}
      GIN_MODE: ${GIN_MODE}
      DECKLOG_ALLOW_INSECURE: ${DECKLOG_ALLOW_INSECURE}
      DECKLOG_TLS_CERT_FILE: ${DECKLOG_TLS_CERT_FILE}
      DECKLOG_TLS_KEY_FILE: ${DECKLOG_TLS_KEY_FILE}
      DECKLOG_METRICS_PORT: ${DECKLOG_METRICS_PORT}
      QUARTERMASTER_URL: ${QUARTERMASTER_URL}
      SERVICE_TOKEN: ${SERVICE_TOKEN}
    ports:
      - "18006:18006"
      - "18026:18026"
    depends_on:
      kafka:
        condition: service_healthy
      kafka-init:
        condition: service_completed_successfully
    restart: unless-stopped
    networks:
      - frameworks
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "grpcurl", "-plaintext", "localhost:18006", "grpc.health.v1.Health/Check"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 15s

  # Signalman - WebSocket Hub Service  
  signalman:
    <<: *default-env
    container_name: frameworks-signalman
    build:
      context: .
      dockerfile: api_realtime/Dockerfile
    hostname: signalman
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy
      quartermaster:
        condition: service_started
    environment:
      PORT: ${SIGNALMAN_PORT}
      DATABASE_URL: ${DATABASE_URL}
      KAFKA_BROKERS: ${KAFKA_BROKERS}
      KAFKA_TOPICS: ${KAFKA_TOPICS}
      KAFKA_GROUP_ID: ${SIGNALMAN_KAFKA_GROUP_ID}
      KAFKA_CLUSTER_ID: ${KAFKA_CLUSTER_ID}
      KAFKA_CLIENT_ID: ${SIGNALMAN_KAFKA_CLIENT_ID}
      LOG_LEVEL: ${LOG_LEVEL}
      REGION: ${REGION}
      QUARTERMASTER_URL: ${QUARTERMASTER_URL}
      JWT_SECRET: ${JWT_SECRET}
      SERVICE_TOKEN: ${SERVICE_TOKEN}
    ports:
      - "18009:18009"
    networks:
      - frameworks
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:18009/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Backend API
  commodore:
    <<: *default-env
    container_name: frameworks-commodore
    build:
      context: .
      dockerfile: api_control/Dockerfile
    environment:
      DATABASE_URL: ${DATABASE_URL}
      JWT_SECRET: ${JWT_SECRET}
      FOGHORN_URL: ${FOGHORN_URL}
      PORT: ${COMMODORE_PORT}
      QUARTERMASTER_URL: ${QUARTERMASTER_URL}
      PURSER_URL: ${PURSER_URL}
      WEBAPP_PUBLIC_URL: ${WEBAPP_PUBLIC_URL}
      SERVICE_TOKEN: ${SERVICE_TOKEN}
      TURNSTILE_AUTH_SECRET_KEY: ${TURNSTILE_AUTH_SECRET_KEY}
    ports:
      - "18001:18001"
    depends_on:
      postgres:
        condition: service_healthy
      quartermaster:
        condition: service_started
    networks:
      - frameworks
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:18001/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Edge Sidecar API
  helmsman:
    <<: *default-env
    container_name: frameworks-helmsman
    build:
      context: .
      dockerfile: api_sidecar/Dockerfile
    environment:
      HELMSMAN_WEBHOOK_URL: ${HELMSMAN_WEBHOOK_URL}
      FOGHORN_URL: ${FOGHORN_URL}
      FOGHORN_CONTROL_ADDR: ${FOGHORN_CONTROL_ADDR}
      EDGE_PUBLIC_HOSTNAME: ${EDGE_PUBLIC_HOSTNAME}
      MISTSERVER_URL: ${MISTSERVER_URL}
      MIST_PASSWORD: ${MIST_PASSWORD}
      MIST_API_USERNAME: ${MIST_API_USERNAME}
      MIST_API_PASSWORD: ${MIST_API_PASSWORD}
      CLUSTER_ID: ${HELMSMAN_CLUSTER_ID}
      NODE_ID: ${HELMSMAN_NODE_ID}
      PORT: ${HELMSMAN_PORT}
      LOG_LEVEL: ${LOG_LEVEL}
      SERVICE_TOKEN: ${SERVICE_TOKEN}
      QUARTERMASTER_URL: ${QUARTERMASTER_URL}
      GRPC_USE_TLS: ${HELMSMAN_GRPC_USE_TLS}  # Set to true for production TLS
      GRPC_TLS_CERT_PATH: ${HELMSMAN_GRPC_TLS_CERT_PATH}
      GRPC_TLS_KEY_PATH: ${HELMSMAN_GRPC_TLS_KEY_PATH}
    volumes:
      - ./infrastructure/edge/machine-id:/etc/machine-id:ro
    depends_on:
      commodore:
        condition: service_started
      foghorn:
        condition: service_started
      mistserver:
        condition: service_started
      decklog:
        condition: service_started
    ports:
      - "18007:18007"
    networks:
      - frameworks
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:18007/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
  
  # Periscope-Ingest: High-throughput event processing from Kafka
  periscope-ingest:
    <<: *default-env
    container_name: frameworks-periscope-ingest
    build:
      context: .
      dockerfile: api_analytics_ingest/Dockerfile
    ports:
      - "18005:18005"
    environment:
      DATABASE_URL: ${DATABASE_URL}
      KAFKA_BROKERS: ${KAFKA_BROKERS}
      KAFKA_GROUP_ID: ${PERISCOPE_INGEST_KAFKA_GROUP_ID}
      KAFKA_CLUSTER_ID: ${KAFKA_CLUSTER_ID}
      KAFKA_TOPICS: ${KAFKA_TOPICS}
      KAFKA_CLIENT_ID: ${PERISCOPE_INGEST_KAFKA_CLIENT_ID}
      LOG_LEVEL: ${LOG_LEVEL}
      ENABLE_HEALTH_ENDPOINT: ${PERISCOPE_INGEST_ENABLE_HEALTH_ENDPOINT}
      HEALTH_PORT: ${PERISCOPE_INGEST_PORT}
      CLICKHOUSE_HOST: ${CLICKHOUSE_HOST}
      CLICKHOUSE_PORT: ${CLICKHOUSE_PORT}
      CLICKHOUSE_DB: ${CLICKHOUSE_DB}
      CLICKHOUSE_USER: ${CLICKHOUSE_USER}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD}
      QUARTERMASTER_URL: ${QUARTERMASTER_URL}
      SERVICE_TOKEN: ${SERVICE_TOKEN}
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy
      kafka-init:
        condition: service_completed_successfully
      clickhouse:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - frameworks
    mem_limit: 1g
    mem_reservation: 512m
    cpus: 2.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:18005/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 15s

  # Periscope-Query (Analytics API - Query Side)
  periscope-query:
    <<: *default-env
    container_name: frameworks-periscope-query
    build:
      context: .
      dockerfile: api_analytics_query/Dockerfile
    ports:
      - "18004:18004"
    environment:
      DATABASE_URL: ${DATABASE_URL}
      PORT: ${PERISCOPE_QUERY_PORT}
      LOG_LEVEL: ${LOG_LEVEL}
      GIN_MODE: ${GIN_MODE}
      JWT_SECRET: ${JWT_SECRET}
      SERVICE_TOKEN: ${SERVICE_TOKEN}
      PURSER_URL: ${PURSER_URL}
      CLICKHOUSE_HOST: ${CLICKHOUSE_HOST}
      CLICKHOUSE_PORT: ${CLICKHOUSE_PORT}
      CLICKHOUSE_DB: ${CLICKHOUSE_DB}
      CLICKHOUSE_USER: ${CLICKHOUSE_USER}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD}
      QUARTERMASTER_URL: ${QUARTERMASTER_URL}
    depends_on:
      postgres:
        condition: service_healthy
      purser:
        condition: service_started
      clickhouse:
        condition: service_healthy
      quartermaster:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - frameworks
    mem_limit: 1g
    mem_reservation: 512m
    cpus: 2.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:18004/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Purser (Billing API)
  purser:
    <<: *default-env
    container_name: frameworks-purser
    build:
      context: .
      dockerfile: api_billing/Dockerfile
    ports:
      - "18003:18003"
    environment:
      DATABASE_URL: ${DATABASE_URL}
      PORT: ${PURSER_PORT}
      LOG_LEVEL: ${LOG_LEVEL}
      STRIPE_SECRET_KEY: ${STRIPE_SECRET_KEY}
      STRIPE_WEBHOOK_SECRET: ${STRIPE_WEBHOOK_SECRET}
      GIN_MODE: ${GIN_MODE}
      JWT_SECRET: ${JWT_SECRET}
      SERVICE_TOKEN: ${SERVICE_TOKEN}
      QUARTERMASTER_URL: ${QUARTERMASTER_URL}
      # Optional: Crypto payment providers (uncomment when needed)
      # - MOLLIE_API_KEY=${MOLLIE_API_KEY}
      # - ETHERSCAN_API_KEY=${ETHERSCAN_API_KEY}
      # - BLOCKCYPHER_API_KEY=${BLOCKCYPHER_API_KEY}
      # - ETHEREUM_RPC_URL=${ETHEREUM_RPC_URL}
      # - MASTER_WALLET_SEED=${MASTER_WALLET_SEED}
      # Optional: Email/SMTP configuration (uncomment when needed)
      # - SMTP_HOST=${SMTP_HOST}
      # - SMTP_PORT=${SMTP_PORT}
      # - SMTP_USER=${SMTP_USER}
      # - SMTP_PASSWORD=${SMTP_PASSWORD}
      # - FROM_EMAIL=${FROM_EMAIL}
      # - FROM_NAME=${FROM_NAME}
      # - WEBAPP_PUBLIC_URL=${WEBAPP_PUBLIC_URL}
    depends_on:
      postgres:
        condition: service_healthy
      quartermaster:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - frameworks
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:18003/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Forms service
  forms:
    <<: *default-env
    container_name: frameworks-forms
    build:
      context: .
      dockerfile: api_forms/Dockerfile
    ports:
      - "18032:18032"
    environment:
      PORT: ${API_FORMS_PORT}
      NODE_ENV: ${NODE_ENV}
      FORMS_ALLOWED_ORIGINS: ${FORMS_ALLOWED_ORIGINS}
      TURNSTILE_FORMS_SECRET_KEY: ${TURNSTILE_FORMS_SECRET_KEY}
      SMTP_HOST: ${SMTP_HOST}
      SMTP_PORT: ${SMTP_PORT}
      SMTP_USER: ${SMTP_USER}
      SMTP_PASSWORD: ${SMTP_PASSWORD}
      FROM_EMAIL: ${FROM_EMAIL}
      TO_EMAIL: ${TO_EMAIL}
    restart: unless-stopped
    networks:
      - frameworks
    mem_limit: 256m
    mem_reservation: 128m
    cpus: 0.5
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:18032/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Web Console
  webapp:
    container_name: frameworks-webapp
    build:
      context: .
      dockerfile: website_application/Dockerfile
      args:
        - BUILD_ENV=${BUILD_ENV}
        - APP_PORT=${WEBAPP_PORT}
        - VITE_AUTH_URL=${VITE_AUTH_URL}
        - VITE_GRAPHQL_HTTP_URL=${VITE_GRAPHQL_HTTP_URL}
        - VITE_GRAPHQL_WS_URL=${VITE_GRAPHQL_WS_URL}
        - VITE_RTMP_DOMAIN=${VITE_RTMP_DOMAIN}
        - VITE_HTTP_DOMAIN=${VITE_HTTP_DOMAIN}
        - VITE_CDN_DOMAIN=${VITE_CDN_DOMAIN}
        - VITE_RTMP_PATH=${VITE_RTMP_PATH}
        - VITE_HLS_PATH=${VITE_HLS_PATH}
        - VITE_WEBRTC_PATH=${VITE_WEBRTC_PATH}
        - VITE_EMBED_PATH=${VITE_EMBED_PATH}
        - VITE_MARKETING_SITE_URL=${VITE_MARKETING_SITE_URL}
        - VITE_TURNSTILE_AUTH_SITE_KEY=${VITE_TURNSTILE_AUTH_SITE_KEY}
        - BASE_PATH=${BASE_PATH}
    depends_on:
      commodore:
        condition: service_started
    environment:
      - NODE_ENV=${NODE_ENV}
      - PORT=${WEBAPP_PORT}
      - HOST=0.0.0.0
    ports:
      - "${WEBAPP_PORT:-18030}:${WEBAPP_PORT:-18030}"
    networks:
      - frameworks
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # Marketing Website
  website:
    container_name: frameworks-website
    build:
      context: .
      dockerfile: website_marketing/Dockerfile
      args:
        - BUILD_ENV=${BUILD_ENV}
        - APP_PORT=${WEBSITE_PORT}
        - VITE_APP_URL=${VITE_APP_URL}
        - VITE_GATEWAY_URL=${VITE_GATEWAY_URL}
        - VITE_GITHUB_URL=${VITE_GITHUB_URL}
        - VITE_LIVEPEER_URL=${VITE_LIVEPEER_URL}
        - VITE_LIVEPEER_EXPLORER_URL=${VITE_LIVEPEER_EXPLORER_URL}
        - VITE_CONTACT_EMAIL=${VITE_CONTACT_EMAIL}
        - VITE_DEMO_STREAM_NAME=${VITE_DEMO_STREAM_NAME}
        - VITE_COMPANY_NAME=${VITE_COMPANY_NAME}
        - VITE_TURNSTILE_FORMS_SITE_KEY=${VITE_TURNSTILE_FORMS_SITE_KEY}
        - VITE_DOMAIN=${VITE_DOMAIN}
        - VITE_CONTACT_API_URL=${VITE_CONTACT_API_URL}
    hostname: website
    environment:
      - NODE_ENV=${NODE_ENV}
      - PORT=${WEBSITE_PORT}
      - HOST=0.0.0.0
    ports:
      - "${WEBSITE_PORT:-18031}:${WEBSITE_PORT:-18031}"
    depends_on:
      bridge:
        condition: service_started
    restart: unless-stopped
    networks:
      - frameworks
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:18031/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Reverse Proxy
  nginx:
    image: nginx:alpine
    container_name: frameworks-nginx
    ports:
      - "18090:80"
    volumes:
      - ./infrastructure/nginx/default.conf:/etc/nginx/conf.d/default.conf:ro
    depends_on:
      bridge:
        condition: service_started
      mistserver:
        condition: service_started
      foghorn:
        condition: service_started
      commodore:
        condition: service_started
      helmsman:
        condition: service_started
      periscope-query:
        condition: service_started
      webapp:
        condition: service_started
      website:
        condition: service_started
      quartermaster:
        condition: service_started
    networks:
      - frameworks
    mem_limit: 256m
    mem_reservation: 128m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:80/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s

  # ClickHouse for time-series analytics
  clickhouse:
    image: clickhouse/clickhouse-server:latest
    container_name: frameworks-clickhouse
    hostname: clickhouse
    ports:
      - "8123:8123"   # HTTP interface
      - "9000:9000"   # Native interface
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ./infrastructure/clickhouse/users.xml:/etc/clickhouse-server/users.xml:ro
      - ./infrastructure/clickhouse/config.xml:/etc/clickhouse-server/config.d/frameworks.xml:ro
      - ./database/init_clickhouse_periscope.sql:/docker-entrypoint-initdb.d/init.sql:ro
    environment:
      # Start with default user for initialization
      CLICKHOUSE_USER: default
      CLICKHOUSE_DB: frameworks
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
      # Environment variables for users.xml
      CLICKHOUSE_PASSWORD: frameworks_dev  # For frameworks user
      CLICKHOUSE_READONLY_PASSWORD: readonly_dev
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    mem_limit: 2g
    mem_reservation: 1g
    cpus: 2.0
    networks:
      - frameworks
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8123/ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Prometheus for metrics collection
  prometheus:
    image: prom/prometheus:latest
    container_name: frameworks-prometheus
    hostname: prometheus
    ports:
      - "9091:9090"
    volumes:
      - prometheus_data:/prometheus
      - ./infrastructure/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./infrastructure/prometheus/rules:/etc/prometheus/rules:ro
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    networks:
      - frameworks
    mem_limit: 1g
    mem_reservation: 512m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: wget --no-verbose --tries=1 --spider http://localhost:9090/-/healthy || exit 1
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Grafana for visualization
  grafana:
    <<: *default-env
    image: grafana/grafana:latest
    container_name: frameworks-grafana
    hostname: grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./infrastructure/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./infrastructure/grafana/dashboards:/var/lib/grafana/dashboards:ro
    environment:
      GF_SECURITY_ADMIN_USER: ${GF_SECURITY_ADMIN_USER}
      GF_SECURITY_ADMIN_PASSWORD: ${GF_SECURITY_ADMIN_PASSWORD}
      GF_USERS_ALLOW_SIGN_UP: "false"
      GF_INSTALL_PLUGINS: grafana-clickhouse-datasource
    depends_on:
      prometheus:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    networks:
      - frameworks
    mem_limit: 512m
    mem_reservation: 256m
    cpus: 1.0
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    healthcheck:
      test: wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 20s
